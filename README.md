# LogicLens

# Automated Answer Validation System (LLM-based)
---

## üîç Overview
This repository contains a novel pipeline for **verifying machine-generated educational answers**.  
Unlike standard RAG similarity-based validation methods, which fail to detect **logical contradictions**, this system introduces a more rigorous method: **Atomic Claim Decomposition**.

This framework is designed to detect *hallucinations of logic*, not just semantic mismatches.

---

## ‚ùó The Core Problem
Traditional answer validation relies on cosine similarity between the answer and the context.  
However, **high similarity does not guarantee logical correctness**.

#### Example

| Context | Candidate Answer | Cosine Similarity Output | My System Output |
|---------|-----------------|-------------------------|-----------------|
| *‚ÄúThe oxygen is released.‚Äù* | *‚ÄúThe oxygen is consumed.‚Äù* | ~0.90 ‚Üí **Incorrectly ‚ÄúCorrect‚Äù** ‚ùå | **Logically Contradictory ‚Üí Incorrect** ‚úÖ |

High similarity ‚â† logical consistency.

---

## üöÄ Key Innovations

### ‚úî 1. Atomic Decomposition
Break long answers into **individual factual claims** for precise evaluation.

### ‚úî 2. Dense Evidence Retrieval (SBERT)
For each atomic claim, retrieve the **most relevant sentence** from the context using SBERT embeddings.

### ‚úî 3. NLI Verification (Cross-Encoder)
Use an NLI model to classify the relationship between the claim and retrieved evidence:
- **Entailment**
- **Contradiction**
- **Neutral**

### ‚úî 4. Explainability
Outputs a **granular, interpretable report** showing:
- Which claims passed/failed  
- Evidence sentences  
- NLI confidence scores  

---

# System Architecture 

1. **Input Answer**  
   The system receives the answer generated by a model or student.

2. **Atomic Decomposition**  
   The answer is broken down into individual factual claims.

3. **Evidence Retrieval**  
   For each atomic claim, the system retrieves the most relevant sentence(s) from the source context.

4. **NLI Verification**  
   Each claim is checked against its retrieved evidence using an NLI (Natural Language Inference) model to classify the relationship as:
   - Entailment
   - Contradiction
   - Neutral

5. **Verdict**  
   Based on the NLI results, each claim (and the overall answer) is classified as:
   - Correct
   - Incorrect
   - Needs Review

6. **Explainable Report**  
   The system outputs a detailed, interpretable report showing:
   - Which claims passed or failed
   - Evidence sentences used
   - NLI confidence scores
